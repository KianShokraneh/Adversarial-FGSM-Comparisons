{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Installing Required Dependecies**"
      ],
      "metadata": {
        "id": "F9QVM5Nc0h7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install foolbox torch torchvision cleverhans torchattacks adversarial-robustness-toolbox"
      ],
      "metadata": {
        "collapsed": true,
        "id": "B4BDna9fOb2_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db0033c3-0060-4d5b-e212-3529e22524e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting foolbox\n",
            "  Downloading foolbox-3.3.4-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.0+cu121)\n",
            "Collecting cleverhans\n",
            "  Downloading cleverhans-4.0.0-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.3/92.3 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchattacks\n",
            "  Downloading torchattacks-3.5.1-py3-none-any.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.0/142.0 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting adversarial-robustness-toolbox\n",
            "  Downloading adversarial_robustness_toolbox-1.17.1-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from foolbox) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from foolbox) (1.11.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from foolbox) (67.7.2)\n",
            "Collecting eagerpy>=0.30.0 (from foolbox)\n",
            "  Downloading eagerpy-0.30.0-py3-none-any.whl (31 kB)\n",
            "Collecting GitPython>=3.0.7 (from foolbox)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.1 in /usr/local/lib/python3.10/dist-packages (from foolbox) (4.11.0)\n",
            "Requirement already satisfied: requests>=2.24.0 in /usr/local/lib/python3.10/dist-packages (from foolbox) (2.31.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Collecting nose (from cleverhans)\n",
            "  Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.7/154.7 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pycodestyle (from cleverhans)\n",
            "  Downloading pycodestyle-2.11.1-py2.py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from cleverhans) (3.7.1)\n",
            "Collecting mnist (from cleverhans)\n",
            "  Downloading mnist-0.2.2-py2.py3-none-any.whl (3.5 kB)\n",
            "Requirement already satisfied: tensorflow-probability in /usr/local/lib/python3.10/dist-packages (from cleverhans) (0.23.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from cleverhans) (1.4.2)\n",
            "Requirement already satisfied: easydict in /usr/local/lib/python3.10/dist-packages (from cleverhans) (1.13)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from cleverhans) (1.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from cleverhans) (1.16.0)\n",
            "Requirement already satisfied: tqdm>=4.56.1 in /usr/local/lib/python3.10/dist-packages (from torchattacks) (4.66.4)\n",
            "Collecting requests>=2.24.0 (from foolbox)\n",
            "  Downloading requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox) (1.2.2)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython>=3.0.7->foolbox)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chardet<5,>=3.0.2 (from requests>=2.24.0->foolbox)\n",
            "  Downloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna<3,>=2.5 (from requests>=2.24.0->foolbox)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting urllib3<1.27,>=1.21.1 (from requests>=2.24.0->foolbox)\n",
            "  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.24.0->foolbox) (2024.2.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->adversarial-robustness-toolbox) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->cleverhans) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->cleverhans) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->cleverhans) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->cleverhans) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->cleverhans) (24.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->cleverhans) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->cleverhans) (2.8.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability->cleverhans) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability->cleverhans) (2.2.1)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability->cleverhans) (0.5.4)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability->cleverhans) (0.1.8)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython>=3.0.7->foolbox)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: nose, urllib3, smmap, pycodestyle, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mnist, idna, eagerpy, chardet, requests, nvidia-cusparse-cu12, nvidia-cudnn-cu12, gitdb, nvidia-cusolver-cu12, GitPython, cleverhans, adversarial-robustness-toolbox, foolbox, torchattacks\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.7\n",
            "    Uninstalling idna-3.7:\n",
            "      Successfully uninstalled idna-3.7\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "bigframes 1.6.0 requires requests>=2.27.1, but you have requests 2.25.1 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.25.1 which is incompatible.\n",
            "tweepy 4.14.0 requires requests<3,>=2.27.0, but you have requests 2.25.1 which is incompatible.\n",
            "yfinance 0.2.40 requires requests>=2.31, but you have requests 2.25.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed GitPython-3.1.43 adversarial-robustness-toolbox-1.17.1 chardet-4.0.0 cleverhans-4.0.0 eagerpy-0.30.0 foolbox-3.3.4 gitdb-4.0.11 idna-2.10 mnist-0.2.2 nose-1.3.7 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 pycodestyle-2.11.1 requests-2.25.1 smmap-5.0.1 torchattacks-3.5.1 urllib3-1.26.18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Fine-tune Resnet18 on CIFAR10**"
      ],
      "metadata": {
        "id": "NsfoWPpW0uYX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jq11AAKXNX8A",
        "outputId": "68675b73-4b75-4f3f-a15b-02f113e81544"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170498071/170498071 [00:03<00:00, 43042919.20it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 174MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Step [100/782], Loss: 0.6080\n",
            "Epoch [1/10], Step [200/782], Loss: 0.6426\n",
            "Epoch [1/10], Step [300/782], Loss: 0.4534\n",
            "Epoch [1/10], Step [400/782], Loss: 0.4303\n",
            "Epoch [1/10], Step [500/782], Loss: 0.3247\n",
            "Epoch [1/10], Step [600/782], Loss: 0.5733\n",
            "Epoch [1/10], Step [700/782], Loss: 0.3175\n",
            "Epoch [2/10], Step [100/782], Loss: 0.2260\n",
            "Epoch [2/10], Step [200/782], Loss: 0.6204\n",
            "Epoch [2/10], Step [300/782], Loss: 0.2094\n",
            "Epoch [2/10], Step [400/782], Loss: 0.3095\n",
            "Epoch [2/10], Step [500/782], Loss: 0.5173\n",
            "Epoch [2/10], Step [600/782], Loss: 0.2774\n",
            "Epoch [2/10], Step [700/782], Loss: 0.1408\n",
            "Epoch [3/10], Step [100/782], Loss: 0.2842\n",
            "Epoch [3/10], Step [200/782], Loss: 0.2344\n",
            "Epoch [3/10], Step [300/782], Loss: 0.1734\n",
            "Epoch [3/10], Step [400/782], Loss: 0.2955\n",
            "Epoch [3/10], Step [500/782], Loss: 0.2753\n",
            "Epoch [3/10], Step [600/782], Loss: 0.1329\n",
            "Epoch [3/10], Step [700/782], Loss: 0.2842\n",
            "Epoch [4/10], Step [100/782], Loss: 0.0737\n",
            "Epoch [4/10], Step [200/782], Loss: 0.1246\n",
            "Epoch [4/10], Step [300/782], Loss: 0.2202\n",
            "Epoch [4/10], Step [400/782], Loss: 0.2035\n",
            "Epoch [4/10], Step [500/782], Loss: 0.2184\n",
            "Epoch [4/10], Step [600/782], Loss: 0.1912\n",
            "Epoch [4/10], Step [700/782], Loss: 0.1731\n",
            "Epoch [5/10], Step [100/782], Loss: 0.0600\n",
            "Epoch [5/10], Step [200/782], Loss: 0.1140\n",
            "Epoch [5/10], Step [300/782], Loss: 0.0795\n",
            "Epoch [5/10], Step [400/782], Loss: 0.1024\n",
            "Epoch [5/10], Step [500/782], Loss: 0.2447\n",
            "Epoch [5/10], Step [600/782], Loss: 0.2072\n",
            "Epoch [5/10], Step [700/782], Loss: 0.1266\n",
            "Epoch [6/10], Step [100/782], Loss: 0.0392\n",
            "Epoch [6/10], Step [200/782], Loss: 0.1125\n",
            "Epoch [6/10], Step [300/782], Loss: 0.0816\n",
            "Epoch [6/10], Step [400/782], Loss: 0.1021\n",
            "Epoch [6/10], Step [500/782], Loss: 0.0510\n",
            "Epoch [6/10], Step [600/782], Loss: 0.1058\n",
            "Epoch [6/10], Step [700/782], Loss: 0.1370\n",
            "Epoch [7/10], Step [100/782], Loss: 0.0478\n",
            "Epoch [7/10], Step [200/782], Loss: 0.0312\n",
            "Epoch [7/10], Step [300/782], Loss: 0.0434\n",
            "Epoch [7/10], Step [400/782], Loss: 0.0578\n",
            "Epoch [7/10], Step [500/782], Loss: 0.0466\n",
            "Epoch [7/10], Step [600/782], Loss: 0.0428\n",
            "Epoch [7/10], Step [700/782], Loss: 0.1031\n",
            "Epoch [8/10], Step [100/782], Loss: 0.0539\n",
            "Epoch [8/10], Step [200/782], Loss: 0.0287\n",
            "Epoch [8/10], Step [300/782], Loss: 0.0037\n",
            "Epoch [8/10], Step [400/782], Loss: 0.1056\n",
            "Epoch [8/10], Step [500/782], Loss: 0.0425\n",
            "Epoch [8/10], Step [600/782], Loss: 0.0375\n",
            "Epoch [8/10], Step [700/782], Loss: 0.1526\n",
            "Epoch [9/10], Step [100/782], Loss: 0.0218\n",
            "Epoch [9/10], Step [200/782], Loss: 0.0054\n",
            "Epoch [9/10], Step [300/782], Loss: 0.0395\n",
            "Epoch [9/10], Step [400/782], Loss: 0.1474\n",
            "Epoch [9/10], Step [500/782], Loss: 0.2265\n",
            "Epoch [9/10], Step [600/782], Loss: 0.1035\n",
            "Epoch [9/10], Step [700/782], Loss: 0.1887\n",
            "Epoch [10/10], Step [100/782], Loss: 0.0485\n",
            "Epoch [10/10], Step [200/782], Loss: 0.0292\n",
            "Epoch [10/10], Step [300/782], Loss: 0.1062\n",
            "Epoch [10/10], Step [400/782], Loss: 0.2246\n",
            "Epoch [10/10], Step [500/782], Loss: 0.0623\n",
            "Epoch [10/10], Step [600/782], Loss: 0.0200\n",
            "Epoch [10/10], Step [700/782], Loss: 0.0831\n",
            "Finished Training\n",
            "Accuracy of the model on the test images: 90.66%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "num_epochs = 10\n",
        "learning_rate = 0.001\n",
        "batch_size = 64\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "])\n",
        "\n",
        "train_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "model = models.resnet18(pretrained=True)\n",
        "model.fc = nn.Linear(model.fc.in_features, 10)\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{total_step}], Loss: {loss.item():.4f}')\n",
        "\n",
        "torch.save(model.state_dict(), 'resnet18_cifar10.pth')\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'Accuracy of the model on the test images: {100 * correct / total}%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Preparing Model & Dataset**"
      ],
      "metadata": {
        "id": "41PLu4dg1Fqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import foolbox as fb\n",
        "from cleverhans.torch.attacks.fast_gradient_method import fast_gradient_method\n",
        "import torchattacks\n",
        "from art.attacks.evasion import FastGradientMethod\n",
        "from art.estimators.classification import PyTorchClassifier\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "dataset = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "model = models.resnet18()\n",
        "model.fc = nn.Linear(model.fc.in_features, 10)\n",
        "model.load_state_dict(torch.load('resnet18_cifar10.pth'))\n",
        "model = model.to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSOBCrxCPjm_",
        "outputId": "33ac48fa-51f0-4eca-f111-b43df93f394e",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **General Functions to Compute ASR**"
      ],
      "metadata": {
        "id": "mkn-Vhh11M6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, images):\n",
        "    logits = model(images)\n",
        "    return logits.argmax(axis=-1)\n",
        "\n",
        "def compute_asr(dataloader, model, attack_fn, epsilon):\n",
        "    total = 0\n",
        "    success = 0\n",
        "\n",
        "    for images, labels in dataloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        clean_predictions = predict(model, images)\n",
        "        adv_images = attack_fn(model, images, labels, epsilon)\n",
        "        adv_predictions = predict(model, adv_images)\n",
        "\n",
        "        success += (clean_predictions != adv_predictions).sum().item()\n",
        "        total += images.size(0)\n",
        "\n",
        "    return success / total"
      ],
      "metadata": {
        "id": "yRQ3T2RVPqVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Attack Functions Using AT Libraries**"
      ],
      "metadata": {
        "id": "Mxfb9Y4i1b8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def foolbox_attack_fn(model, images, labels, epsilon):\n",
        "    fmodel = fb.PyTorchModel(model, bounds=(0, 1))\n",
        "    attack = fb.attacks.FGSM()\n",
        "    adv_images, _, _ = attack(fmodel, images, labels, epsilons=epsilon)\n",
        "    return adv_images\n",
        "\n",
        "def cleverhans_attack_fn(model, images, labels, epsilon):\n",
        "    adv_images = fast_gradient_method(model, images, epsilon, np.inf)\n",
        "    return adv_images\n",
        "\n",
        "def torchattacks_attack_fn(model, images, labels, epsilon):\n",
        "    attack = torchattacks.FGSM(model, eps=epsilon)\n",
        "    adv_images = attack(images, labels)\n",
        "    return adv_images\n",
        "\n",
        "def art_attack_fn(model, images, labels, epsilon):\n",
        "    classifier = PyTorchClassifier(\n",
        "        model=model,\n",
        "        clip_values=(0, 1),\n",
        "        loss=nn.CrossEntropyLoss(),\n",
        "        optimizer=torch.optim.Adam(model.parameters(), lr=0.01),\n",
        "        input_shape=(3, 224, 224),\n",
        "        nb_classes=10,\n",
        "    )\n",
        "    attack = FastGradientMethod(estimator=classifier, eps=epsilon)\n",
        "    adv_images = attack.generate(x=images.cpu().numpy())\n",
        "    adv_images = torch.tensor(adv_images).to(device)\n",
        "    return adv_images\n",
        "\n",
        "epsilon = 0.03"
      ],
      "metadata": {
        "id": "bGt_K9nuV5gA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ASR using Foolbox**"
      ],
      "metadata": {
        "id": "cezb0Ye51eFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "asr_foolbox = compute_asr(dataloader, model, foolbox_attack_fn, epsilon)\n",
        "print(f'(ASR) with Foolbox FGSM and epsilon {epsilon}: {asr_foolbox * 100:.2f}%')\n",
        "\n"
      ],
      "metadata": {
        "id": "L832GPGIPyj0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02313fd8-87eb-4ec7-f83a-bc684225c6f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(ASR) with Foolbox FGSM and epsilon 0.03: 61.46%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ASR using CleverHans**"
      ],
      "metadata": {
        "id": "iUFF9Sr11oV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "asr_cleverhans = compute_asr(dataloader, model, cleverhans_attack_fn, epsilon)\n",
        "print(f'(ASR) with CleverHans FGSM and epsilon {epsilon}: {asr_cleverhans * 100:.2f}%')\n",
        "\n"
      ],
      "metadata": {
        "id": "wqC2gbByPyd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad686b41-a84e-4534-ff25-e78049017424"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(ASR) with CleverHans FGSM and epsilon 0.03: 95.59%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ASR using Torchattacks**"
      ],
      "metadata": {
        "id": "wBtAFQsh1rJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "asr_torchattacks = compute_asr(dataloader, model, torchattacks_attack_fn, epsilon)\n",
        "print(f'(ASR) with Torchattacks FGSM and epsilon {epsilon}: {asr_torchattacks * 100:.2f}%')\n",
        "\n"
      ],
      "metadata": {
        "id": "jq8zmhv0PyXA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3a9d5ef-741b-47af-8cd7-483b03728663"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(ASR) with Torchattacks FGSM and epsilon 0.03: 61.46%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ASR using ART**"
      ],
      "metadata": {
        "id": "fCiUUvBI1zSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "asr_art = compute_asr(dataloader, model, art_attack_fn, epsilon)\n",
        "print(f'(ASR) with ART FGSM and epsilon {epsilon}: {asr_art * 100:.2f}%')\n"
      ],
      "metadata": {
        "id": "hDxTWdE1PyPL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ff4e8ec-12ed-4f38-baec-cd9eb9e9baeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(ASR) with ART FGSM and epsilon 0.03: 95.65%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Realizing Attack Success Using A Sample Input**"
      ],
      "metadata": {
        "id": "6ghHb3gc15es"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import DataLoader\n",
        "import foolbox as fb\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = models.resnet18()\n",
        "model.fc = nn.Linear(model.fc.in_features, 10)\n",
        "model.load_state_dict(torch.load('resnet18_cifar10.pth'))\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "dataset = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "image, label = next(iter(dataloader))\n",
        "image, label = image.to(device), torch.tensor([label]).to(device)\n",
        "\n",
        "\n",
        "fmodel = fb.PyTorchModel(model, bounds=(0, 1))\n",
        "clean_logits = fmodel(image)\n",
        "clean_prediction = clean_logits.argmax(axis=-1)\n",
        "\n",
        "true_label = label.item()\n",
        "predicted_label = clean_prediction.item()\n",
        "class_names = dataset.classes\n",
        "print(f'True label: {class_names[true_label]} ({true_label})')\n",
        "print(f'Clean prediction: {class_names[predicted_label]} ({predicted_label})')\n",
        "\n",
        "attack = fb.attacks.LinfFastGradientAttack()\n",
        "epsilon = 0.03\n",
        "adv_image, _, _ = attack(fmodel, image, label, epsilons=epsilon)\n",
        "\n",
        "adv_logits = fmodel(adv_image)\n",
        "adv_prediction = adv_logits.argmax(axis=-1)\n",
        "adv_predicted_label = adv_prediction.item()\n",
        "print(f'Adversarial prediction: {class_names[adv_predicted_label]} ({adv_predicted_label})')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSi_cQXhpTK5",
        "outputId": "75052e44-2f82-4651-d5b1-72a99721d3eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "True label: cat (3)\n",
            "Clean prediction: cat (3)\n",
            "Adversarial prediction: deer (4)\n"
          ]
        }
      ]
    }
  ]
}